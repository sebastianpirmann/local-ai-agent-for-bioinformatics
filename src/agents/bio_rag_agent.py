from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from src.core.llm_handler import LLMHandler
from src.core.embedding_handler import EmbeddingHandler
from src.knowledge_base.vector_database import VectorDatabase
from config import (
    OLLAMA_LLM_MODEL,
    OLLAMA_EMBEDDING_MODEL,
    CHROMA_DB_PATH,
    CONTEXT_MODE,
)
from typing import List


class BioRAGAgent:
    """
    A local RAG (Retrieval-Augmented Generation) agent for bioinformatics,
    which queries a local knowledge base and uses a local LLM to answer questions.
    """

    def __init__(self):
        """
        Initializes the RAG agent by setting up the LLM, embedding model,
        and loading the vector database.
        """
        print("Initializing BioRAGAgent...")
        self.context_mode = CONTEXT_MODE

        # Initialize LLM
        llm_handler = LLMHandler(OLLAMA_LLM_MODEL)
        self.llm = llm_handler.get_llm()  # Using the getter method

        # Initialize Embedding Handler and load Vector Database
        embedding_handler = EmbeddingHandler(OLLAMA_EMBEDDING_MODEL)
        embeddings_model = (
            embedding_handler.get_embedding_model()
        )  # Using the getter method

        self.db_handler = VectorDatabase(
            embeddings_model=embeddings_model, persist_directory=CHROMA_DB_PATH
        )
        self.retriever = self._initialize_retriever()

        # Define the RAG prompt template
        self.rag_prompt_strict = ChatPromptTemplate.from_template(
            """Answer the question based ONLY on the following context. 
            If the answer cannot be found in the context, truthfully state that you don't know.
            Provide your answer in a clear and concise manner, suitable for a bioinformatics context.

            Context:
            {context}

            Question: {question}
            """
        )

        self.rag_prompt_regular = ChatPromptTemplate.from_template(
            """You are a helpful bioinformatics assistant. Answer the user's question precisely and accurately.

                **Priority 1: Context from Knowledge Base**
                ALWAYS and exclusively use the information from the 'Context' below, IF it is relevant and directly answers the question.

                **Priority 2: LLM's General Knowledge**
                If the 'Context' below does NOT contain sufficient information to answer the question, THEN and only then, use your general bioinformatics knowledge to answer. Ensure the highest accuracy and do not invent any information.

                **Source Indication (Optional):**
                If you use information from your general knowledge because the context was insufficient, you may briefly indicate this (e.g., "Based on general knowledge...").

                Context:
                {context}

                Question: {question}
                """
        )

        # Define the RAG chain
        # The chain takes the question, retrieves context, formats it, and passes to LLM
        self.rag_chain = (
            {
                "context": self.retriever | RunnableLambda(self._format_docs),
                "question": RunnablePassthrough(),
            }
            | (
                self.rag_prompt_strict
                if self.context_mode == "STRICT"
                else self.rag_prompt_regular
            )
            | self.llm
            | StrOutputParser()
        )

        print("BioRAGAgent initialized successfully.")
        print(f"Using LLM: {OLLAMA_LLM_MODEL}")
        print(f"Using Embedding Model: {OLLAMA_EMBEDDING_MODEL}")
        print(f"Knowledge Base path: {CHROMA_DB_PATH}")

    def _initialize_retriever(self):
        """
        Loads the ChromaDB and creates a retriever instance.
        """
        # Initialize the database. It should load an existing one.
        db = self.db_handler.initialize_db()
        if db:
            return db.as_retriever()
        else:
            raise ValueError(
                f"Could not load vector database from {CHROMA_DB_PATH}. Please ensure it has been built using build_knowledge_base.py"
            )

    def _format_docs(self, docs: List) -> str:
        """
        Formats the retrieved documents into a single string for the LLM context.
        Args:
            docs (List): A list of LangChain Document objects.
        Returns:
            str: A concatenated string of document contents.
        """
        return "\n\n---\n\n".join(doc.page_content for doc in docs)

    def query_agent(self, question: str) -> str:
        """
        Queries the RAG agent with a given question.
        Args:
            question (str): The user's question.
        Returns:
            str: The answer generated by the LLM based on retrieved context.
        """
        # print(f"\n--- Querying agent with: '{question}' ---")
        response = self.rag_chain.invoke(question)
        # print("--- Query Finished ---")
        return response


# Example usage for testing purposes
if __name__ == "__main__":
    # IMPORTANT: Ensure knowledge base is built before running this test

    try:
        agent = BioRAGAgent()

        print("\n--- Test Query 1: Basic Bioinformatics Question ---")
        answer1 = agent.query_agent("What is CRISPR-Cas9?")
        print(f"Answer: {answer1}")

        print("\n--- Test Query 2: Specific Code Question ---")
        answer2 = agent.query_agent(
            "How do you calculate GC content in a DNA sequence?"
        )
        print(f"Answer: {answer2}")

        print(
            "\n--- Test Query 3: Question not in context (should say 'I don't know') ---"
        )
        answer3 = agent.query_agent("What is the capital of France?")
        print(f"Answer: {answer3}")

    except ValueError as e:
        print(f"Error: {e}")
        print(
            "\nPlease ensure your ChromaDB is built by running: python3 -m src.knowledge_base.build_knowledge_base"
        )
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        print(
            "Please ensure Ollama is running and models are pulled (mistral, nomic-embed-text)."
        )
